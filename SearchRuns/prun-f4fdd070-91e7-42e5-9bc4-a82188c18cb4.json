{
    "id": "prun-f4fdd070-91e7-42e5-9bc4-a82188c18cb4",
    "plan_id": "plan-da50f235-a363-47f1-b600-cc3d55bd1c54",
    "current_step_index": 2,
    "state": "NEED_CLARIFICATION",
    "execution_context": {
        "end_user_id": null,
        "additional_data": {},
        "planning_agent_system_context_extension": null,
        "execution_agent_system_context_extension": null
    },
    "outputs": {
        "clarifications": [
            {
                "id": "clar-7feb1787-5e21-4fe4-9315-6342cbfc8bf9",
                "plan_run_id": "prun-f4fdd070-91e7-42e5-9bc4-a82188c18cb4",
                "category": "Input",
                "response": null,
                "step": 2,
                "user_guidance": "Missing Argument: pagename",
                "resolved": false,
                "argument_name": "pagename"
            }
        ],
        "step_outputs": {
            "$video_details": {
                "value": "https://www.youtube.com/watch?v=PeMlggyqz0Y : machine learning teach a computer how to perform a task without explicitly programming it to perform said task instead feed data into an algorithm to gradually improve outcomes with experience similar to how organic life learns the term was coined in 1959 by Arthur Samuel at IBM who is developing artificial intelligence that could play checkers half a century later and predictive models are embedded in many of the products we use every day which perform two fundamental jobs one is to classify data like is there another car on the road or does this patient have cancer the other is to make predictions about future outcomes like will the stock go up or which YouTube video do you want to watch next the first step in the process is to acquire and clean up data lots and lots of data the better the data represents the problem the better the results garbage in garbage out the data needs to have some kind of signal to be valuable to the algorithm for making predictions and data scientists perform a job called feature engineering to transform raw data into features that better represent the underlying problem the next step is to separate the data into a training set and testing set the training data is fed into an algorithm to build a model then the testing data is used to validate the accuracy or error of the model the next step is to choose an algorithm which might be a simple statistical model like linear or logistic regression or a decision tree that assigns different weights to features in the data or you might get fancy with a convolutional neural network which is an algorithm that also assigns weights to Features but also takes the input data and creates a additional features automatically and that's extremely useful for data sets that contain things like images or natural language where manual feature engineering is virtually impossible every one of these algorithms learns to get better by comparing its predictions to an error function if it's a classification problem like is this animal a cat or a dog the error function might be accuracy if it's a regression problem like how much will a loaf of bread cost next year then it might be mean absolute error python is the language of choice among data scientists but R and Julia are also popular options and there are many supporting Frameworks out there to make the process approachable the end result of the machine learning process is a model which is just a file that takes some input data in the same shape that it was trained on then spits out a prediction that tries to minimize the error that it was optimized for it can then be embedded on an actual device or deployed to the cloud to build a real world product this has been machine learning in 100 seconds like And subscribe if you want to see more short videos like this and leave a comment if you want to see more machine learning content on this channel thanks for watching and I will see you in the next one",
                "summary": "https://www.youtube.com/watch?v=PeMlggyqz0Y : machine learning teach a computer how to perform a task without explicitly programming it to perform said task instead feed data into an algorithm to gradually improve outcomes with experience similar to how organic life learns the term was coined in 1959 by Arthur Samuel at IBM who is developing artificial intelligence that could play checkers half a century later and predictive models are embedded in many of the products we use every day which perform two fundamental jobs one is to classify data like is there another car on the road or does this patient have cancer the other is to make predictions about future outcomes like will the stock go up or which YouTube video do you want to watch next the first step in the process is to acquire and clean up data lots and lots of data the better the data represents the problem the better the results garbage in garbage out the data needs to have some kind of signal to be valuable to the algorithm for making predictions and data scientists perform a job called feature engineering to transform raw data into features that better represent the underlying problem the next step is to separate the data into a training set and testing set the training data is fed into an algorithm to build a model then the testing data is used to validate the accuracy or error of the model the next step is to choose an algorithm which might be a simple statistical model like linear or logistic regression or a decision tree that assigns different weights to features in the data or you might get fancy with a convolutional neural network which is an algorithm that also assigns weights to Features but also takes the input data and creates a additional features automatically and that's extremely useful for data sets that contain things like images or natural language where manual feature engineering is virtually impossible every one of these algorithms learns to get better by comparing its predictions to an error function if it's a classification problem like is this animal a cat or a dog the error function might be accuracy if it's a regression problem like how much will a loaf of bread cost next year then it might be mean absolute error python is the language of choice among data scientists but R and Julia are also popular options and there are many supporting Frameworks out there to make the process approachable the end result of the machine learning process is a model which is just a file that takes some input data in the same shape that it was trained on then spits out a prediction that tries to minimize the error that it was optimized for it can then be embedded on an actual device or deployed to the cloud to build a real world product this has been machine learning in 100 seconds like And subscribe if you want to see more short videos like this and leave a comment if you want to see more machine learning content on this channel thanks for watching and I will see you in the next one"
            },
            "$learning_note": {
                "value": "Machine learning enables computers to perform tasks without explicit programming by using data to improve outcomes through algorithms, as explained in the video. The process begins with acquiring and cleaning data, followed by feature engineering to transform raw data into useful features. This data is then divided into training and testing sets to build and validate models. Various algorithms, from simple statistical models like linear regression to complex ones like convolutional neural networks, are used depending on the problem. These algorithms learn by comparing predictions to an error function, such as accuracy for classification problems or mean absolute error for regression problems. Python is the preferred language among data scientists, with R and Julia also being popular, supported by numerous frameworks. The ultimate goal is to create a model that can be deployed in real-world applications, either on devices or in the cloud.",
                "summary": "Machine learning enables computers to perform tasks without explicit programming by using data to improve outcomes through algorithms, as explained in the video. The process begins with acquiring and cleaning data, followed by feature engineering to transform raw data into useful features. This data is then divided into training and testing sets to build and validate models. Various algorithms, from simple statistical models like linear regression to complex ones like convolutional neural networks, are used depending on the problem. These algorithms learn by comparing predictions to an error function, such as accuracy for classification problems or mean absolute error for regression problems. Python is the preferred language among data scientists, with R and Julia also being popular, supported by numerous frameworks. The ultimate goal is to create a model that can be deployed in real-world applications, either on devices or in the cloud."
            },
            "$notion_page": {
                "value": "[{\"id\": \"clar-7feb1787-5e21-4fe4-9315-6342cbfc8bf9\", \"plan_run_id\": \"prun-f4fdd070-91e7-42e5-9bc4-a82188c18cb4\", \"category\": \"Input\", \"response\": null, \"step\": 2, \"user_guidance\": \"Missing Argument: pagename\", \"resolved\": false, \"argument_name\": \"pagename\"}]",
                "summary": null
            }
        },
        "final_output": null
    }
}